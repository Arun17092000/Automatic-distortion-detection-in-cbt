{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hYUUz2ZBgYdM"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Loading Dataset\n",
        "df = pd.read_csv('Dataset_Cognitive_Distortions.tsv', sep='\\t', header=0)\n",
        "#df.shape\n",
        "\n",
        "# Divide data between training and test data\n",
        "X = df['Phrase']\n",
        "y = df['Cognitive Distortion']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp0IV-MfgYdS"
      },
      "source": [
        "MULTINOMIAL NAIVE BAYES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe0MzYxqgYdV",
        "outputId": "dece408c-fd2e-4c8a-fdab-f93c30108db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n",
            "Train Accuracy : 0.994\n",
            "Test Accuracy : 0.689\n",
            "Best Accuracy Through Grid Search : 0.683\n",
            "Best Parameters :  {'mnb__alpha': 0.1, 'smote__k_neighbors': 10}\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  5  0  1  0  0  0  0  0  0  0  0  0  1  0  0]\n",
            " [ 0  0  5  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
            " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  3  1  0  1  0  1  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  6  0  1  0  2  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  1  5  0  0  0  0  0  0]\n",
            " [ 0  0  0  2  0  0  0  2  0  0  4  0  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  1  0  1  1  4  1  1  2  0]\n",
            " [ 0  1  0  1  0  1  0  0  0  0  0  0  5  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  1  0  0  0  1  0  3  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "     Always Being Right       1.00      1.00      1.00         3\n",
            "                Blaming       0.62      0.71      0.67         7\n",
            "        Catastrophizing       0.83      0.83      0.83         6\n",
            "      Control Fallacies       0.64      0.54      0.58        13\n",
            "    Emotional Reasoning       1.00      1.00      1.00        11\n",
            "      Fallacy Of Change       0.67      1.00      0.80         2\n",
            "    Fallacy Of Fairness       1.00      0.50      0.67         6\n",
            "              Filtering       0.50      0.60      0.55        10\n",
            "       Global Labelling       0.82      1.00      0.90         9\n",
            "Heaven's Reward Fallacy       0.56      0.83      0.67         6\n",
            " Jumping to Conclusions       0.80      0.44      0.57         9\n",
            "                     No       0.44      0.36      0.40        11\n",
            "     Overgeneralization       0.71      0.62      0.67         8\n",
            "        Personalization       0.71      1.00      0.83         5\n",
            "              Polarized       0.33      0.50      0.40         6\n",
            "                Shoulds       1.00      0.71      0.83         7\n",
            "\n",
            "               accuracy                           0.69       119\n",
            "              macro avg       0.73      0.73      0.71       119\n",
            "           weighted avg       0.71      0.69      0.69       119\n",
            "\n",
            "0.6890756302521008\n"
          ]
        }
      ],
      "source": [
        "textclassifier = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=0)),\n",
        "    ('mnb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Hyperparameters to tune\n",
        "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10],\n",
        "          'mnb__alpha': [0.01, 0.1, 0.3, 0.5, 1.0]\n",
        "         }\n",
        "\n",
        "# Hyperprameters tuning\n",
        "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
        "multinomial_nb_grid.fit(X_train, y_train)\n",
        "\n",
        "# Print results with 3 decimals\n",
        "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
        "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
        "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
        "\n",
        "y_pred = multinomial_nb_grid.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_c9SMsgYdW"
      },
      "source": [
        "MULTINOMIAL LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN5jEArtgYdX",
        "outputId": "60d04d51-1040-4824-acca-069a60152b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "2700 fits failed out of a total of 7200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "450 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/imblearn/pipeline.py\", line 297, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.07562057 0.0777039  0.0777039\n",
            " 0.0777039  0.0777039  0.0777039  0.0777039  0.0777039  0.0777039\n",
            " 0.56724291 0.55882092 0.56733156 0.57353723 0.57384752 0.58191489\n",
            " 0.58009752 0.57367021 0.59060284 0.56724291 0.55882092 0.56733156\n",
            " 0.57353723 0.57384752 0.58191489 0.58009752 0.57367021 0.59060284\n",
            " 0.56099291 0.55673759 0.56737589 0.57140957 0.57176418 0.58191489\n",
            " 0.58847518 0.56733156 0.59268617 0.52544326 0.51901596 0.51910461\n",
            " 0.53169326 0.51919326 0.52109929 0.52969858 0.52522163 0.5402039\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.07367021 0.06941489 0.07575355\n",
            " 0.07579787 0.08195922 0.07575355 0.07371454 0.08413121 0.06941489\n",
            " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
            " 0.67437943 0.67637411 0.67659574 0.67859043 0.67646277 0.68280142\n",
            " 0.67655142 0.68488475 0.68067376 0.68701241 0.68692376 0.68071809\n",
            " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
            " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
            " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.05465426 0.0545656  0.06103723\n",
            " 0.05678191 0.0545656  0.05886525 0.0483156  0.05682624 0.05669326\n",
            " 0.57570922 0.56932624 0.57783688 0.58404255 0.58435284 0.59033688\n",
            " 0.58847518 0.58413121 0.60115248 0.57570922 0.56932624 0.57783688\n",
            " 0.58404255 0.58435284 0.59033688 0.58847518 0.58413121 0.60115248\n",
            " 0.57570922 0.56932624 0.57783688 0.58195922 0.5822695  0.59033688\n",
            " 0.58847518 0.58413121 0.60115248 0.57575355 0.56307624 0.57570922\n",
            " 0.57566489 0.57805851 0.58191489 0.57792553 0.58204787 0.5947695\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.09862589 0.10088652 0.09237589\n",
            " 0.1008422  0.09875887 0.1008422  0.09875887 0.09875887 0.1008422\n",
            " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
            " 0.67437943 0.67637411 0.67659574 0.67859043 0.67646277 0.68280142\n",
            " 0.67655142 0.68488475 0.68067376 0.68701241 0.68692376 0.68071809\n",
            " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
            " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
            " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.16369681 0.15527482 0.16374113\n",
            " 0.16578014 0.16369681 0.1554078  0.13860816 0.14911348 0.16573582\n",
            " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
            " 0.58838652 0.5945922  0.61578014 0.59680851 0.59024823 0.58408688\n",
            " 0.58617021 0.59689716 0.60079787 0.58838652 0.5945922  0.61578014\n",
            " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
            " 0.58838652 0.5945922  0.61578014 0.59468085 0.58603723 0.58200355\n",
            " 0.58617021 0.59902482 0.60496454 0.59671986 0.59667553 0.6179078\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.3106383  0.32544326 0.27096631\n",
            " 0.29401596 0.31697695 0.3043883  0.2918883  0.29809397 0.30434397\n",
            " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
            " 0.67437943 0.67637411 0.67659574 0.67859043 0.67646277 0.68280142\n",
            " 0.67655142 0.68488475 0.68067376 0.68701241 0.68692376 0.68071809\n",
            " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
            " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
            " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.32101064 0.32960993 0.28125\n",
            " 0.30873227 0.28351064 0.29175532 0.28156028 0.29175532 0.28320035\n",
            " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
            " 0.60305851 0.60305851 0.63049645 0.60296986 0.60279255 0.60718085\n",
            " 0.60088652 0.60939716 0.60921986 0.60305851 0.60305851 0.63049645\n",
            " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
            " 0.60305851 0.60305851 0.63049645 0.60301418 0.59862589 0.60509752\n",
            " 0.60500887 0.6114805  0.61130319 0.6072695  0.60509752 0.63049645\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.41143617 0.40935284 0.38661348\n",
            " 0.39268617 0.39507979 0.40323582 0.38444149 0.38639184 0.39073582\n",
            " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
            " 0.67437943 0.67637411 0.67659574 0.67859043 0.67646277 0.68280142\n",
            " 0.67655142 0.68488475 0.68067376 0.68701241 0.68692376 0.68071809\n",
            " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
            " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
            " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.45602837 0.4623227  0.44135638\n",
            " 0.43275709 0.43289007 0.43927305 0.44140071 0.45164007 0.42646277\n",
            " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
            " 0.64069149 0.65124113 0.66609043 0.64490248 0.64277482 0.64924645\n",
            " 0.64281915 0.63674645 0.63005319 0.64069149 0.65124113 0.66609043\n",
            " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
            " 0.64069149 0.65124113 0.66609043 0.64703014 0.64069149 0.64924645\n",
            " 0.64281915 0.63253546 0.62796986 0.63860816 0.64911348 0.66187943\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.51050532 0.50629433 0.49804965\n",
            " 0.48949468 0.49818262 0.50629433 0.50234929 0.4981383  0.50647163\n",
            " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
            " 0.67437943 0.67637411 0.67659574 0.67859043 0.67646277 0.68280142\n",
            " 0.67655142 0.68488475 0.68067376 0.68701241 0.68692376 0.68071809\n",
            " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
            " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
            " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.998\n",
            "Test Accuracy : 0.647\n",
            "Best Accuracy Through Grid Search : 0.687\n",
            "Best Parameters :  {'mlg__C': 0.01, 'mlg__penalty': 'none', 'mlg__solver': 'saga', 'smote__k_neighbors': 2}\n",
            "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  3  0  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
            " [ 0  0  5  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
            " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  1  0  2  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  4  0  1  0  4  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  0  5  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  1  0  0  0  0  0  5  0  1  0  1  0]\n",
            " [ 0  0  0  0  0  0  1  1  0  1  1  4  1  1  1  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  1  0  6  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
            " [ 0  0  1  1  0  0  0  1  0  0  0  0  1  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "     Always Being Right       0.75      1.00      0.86         3\n",
            "                Blaming       0.60      0.43      0.50         7\n",
            "        Catastrophizing       0.71      0.83      0.77         6\n",
            "      Control Fallacies       0.70      0.54      0.61        13\n",
            "    Emotional Reasoning       0.92      1.00      0.96        11\n",
            "      Fallacy Of Change       1.00      1.00      1.00         2\n",
            "    Fallacy Of Fairness       0.25      0.17      0.20         6\n",
            "              Filtering       0.50      0.40      0.44        10\n",
            "       Global Labelling       1.00      1.00      1.00         9\n",
            "Heaven's Reward Fallacy       0.45      0.83      0.59         6\n",
            " Jumping to Conclusions       0.71      0.56      0.63         9\n",
            "                     No       0.36      0.36      0.36        11\n",
            "     Overgeneralization       0.60      0.75      0.67         8\n",
            "        Personalization       0.71      1.00      0.83         5\n",
            "              Polarized       0.29      0.33      0.31         6\n",
            "                Shoulds       1.00      0.71      0.83         7\n",
            "\n",
            "               accuracy                           0.65       119\n",
            "              macro avg       0.66      0.68      0.66       119\n",
            "           weighted avg       0.65      0.65      0.64       119\n",
            "\n",
            "0.6470588235294118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "textclassifier = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=0)),\n",
        "    ('mlg', LogisticRegression( multi_class='multinomial', # Set algorithm to multinomial since this is a multi class problem\n",
        "                                random_state=0, # Set the seed for the algorithm to make sure there is consistency across multiple runs\n",
        "                                warm_start = True, # To make the model converge faster\n",
        "                                l1_ratio = 0.5)) # Set to the elastic net regularization\n",
        "])\n",
        "\n",
        "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10],\n",
        "          'mlg__penalty': ['l1', 'l2', 'elasticnet', 'none' ],\n",
        "          'mlg__C': [0.01, 0.1, 0.3, 0.5, 1.0,],\n",
        "          'mlg__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'] # List of compatible solver for multi_class = multinomial\n",
        "         }\n",
        "\n",
        "multinomial_lg_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
        "multinomial_lg_grid.fit(X_train, y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%multinomial_lg_grid.best_estimator_.score(X_train, y_train))\n",
        "print('Test Accuracy : %.3f'%multinomial_lg_grid.best_estimator_.score(X_test, y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%multinomial_lg_grid.best_score_)\n",
        "print('Best Parameters : ',multinomial_lg_grid.best_params_)\n",
        "\n",
        "y_pred = multinomial_lg_grid.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "297e491db09c8e1cad3dcb7da858ce4188d439da100fd9118e665979685ce609"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}